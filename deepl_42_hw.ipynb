{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agoston03/Deep-Learning-42/blob/main/deepl_42_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaVr3Q6x0tKL"
      },
      "source": [
        "This is a homework project in \"Deep Learning a gyakorlatban Python és Lua alapokon\".  \n",
        "The team members are:\n",
        "\n",
        "* Gyulai Gergő László\n",
        "* Horváth Ágoston\n",
        "* Frink Dávid\n",
        "\n",
        "You can read more information about our chosen homework at the link below:  \n",
        "https://www.kaggle.com/competitions/isic-2024-challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTfguNqP_MPw"
      },
      "source": [
        "# Download and setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIs7lHT5_R75"
      },
      "source": [
        "Download Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mF86X9w98eu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf64d3d-e28a-4564-9a83-b9b47cba95bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.12\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle==1.5.12) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle==1.5.12) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle==1.5.12) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle==1.5.12) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle==1.5.12) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle==1.5.12) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle==1.5.12) (2.2.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle==1.5.12) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle==1.5.12) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle==1.5.12) (3.10)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73026 sha256=e0f30647ad2b67990c68b8d98166a2c4e5356d5189405045c9645f32e8886a73\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/0c/e6/79103212a102e78b8453691b905f48000219574ba7137e7207\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.6.17\n",
            "    Uninstalling kaggle-1.6.17:\n",
            "      Successfully uninstalled kaggle-1.6.17\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3uje-t7_USk"
      },
      "source": [
        "Configure Kaggle to access the API  \n",
        "**Warning!** You need to copy your own kaggle.json file into Colab in order to valide yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH6K85Qn9_k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea082d1-4c02-4533-9ba9-3591235bff3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcy_RfSI_b3V"
      },
      "source": [
        "Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roNx7yrj-PeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96bd959b-d89f-4736-b8e7-cdf77c2435a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c isic-2024-challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPTbkCYo_g3Z"
      },
      "source": [
        "Unpacking the data  \n",
        "**Warning!** This might take a few minuttes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlKXjhg4_IC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d63f7b3-f843-41e5-e4d5-c0511c29cd1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open isic-2024-challenge.zip, isic-2024-challenge.zip.zip or isic-2024-challenge.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip isic-2024-challenge.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAKVH9CsyP7N"
      },
      "source": [
        "# Preparing train, test and valid set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gather information about the dataset based on the metadata"
      ],
      "metadata": {
        "id": "kiIoGH76cAo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KZGq3a8Y3P0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv('train-metadata.csv')\n",
        "metadata.head()"
      ],
      "metadata": {
        "id": "zkYDYcfg2_pO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "a679cd8b-d65c-4b66-a70a-7e58185d431d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train-metadata.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4c180bfb1031>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-metadata.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the number of benign and malignant data"
      ],
      "metadata": {
        "id": "4ydBP9ivcJPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benign_data = metadata[metadata['target'] == 0]\n",
        "malignant_data = metadata[metadata['target'] == 1]\n",
        "\n",
        "print(f'Benign images: {len(benign_data)}')\n",
        "print(f'Malignant images: {len(malignant_data)}')"
      ],
      "metadata": {
        "id": "rZbBKaex6FMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benign_ids = benign_data['isic_id'].tolist()\n",
        "malignant_ids = malignant_data['isic_id'].tolist()\n",
        "\n",
        "print(f'Benign images ids: {benign_ids[:5]}')\n",
        "print(f'Malignant images ids: {malignant_ids[:5]}')"
      ],
      "metadata": {
        "id": "Nz5LWD6678Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_split = 0.1\n",
        "valid_split = 0.1\n",
        "\n",
        "test_benign_ids = benign_ids[:int(len(benign_ids) * test_split)]\n",
        "test_malignant_ids = malignant_ids[:int(len(malignant_ids) * test_split)]\n",
        "\n",
        "valid_benign_ids = benign_ids[int(len(benign_ids) * test_split):int(len(benign_ids) * (test_split + valid_split))]\n",
        "valid_malignant_ids = malignant_ids[int(len(malignant_ids) * test_split):int(len(malignant_ids) * (test_split + valid_split))]\n",
        "\n",
        "train_benign_ids = benign_ids[int(len(benign_ids) * (test_split + valid_split)):]\n",
        "train_malignant_ids = malignant_ids[int(len(malignant_ids) * (test_split + valid_split)):]\n",
        "\n",
        "print(f'Test benign images: {len(test_benign_ids)}')\n",
        "print(f'Test malignant images: {len(test_malignant_ids)}')\n",
        "print(f'Valid benign images: {len(valid_benign_ids)}')\n",
        "print(f'Valid malignant images: {len(valid_malignant_ids)}')\n",
        "print(f'Train benign images: {len(train_benign_ids)}')\n",
        "print(f'Train malignant images: {len(train_malignant_ids)}')"
      ],
      "metadata": {
        "id": "bqTS4zp8lhIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loadig the images"
      ],
      "metadata": {
        "id": "943_QU1I-vle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare to load and show images"
      ],
      "metadata": {
        "id": "xRsfJD6qcWYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import io\n",
        "import matplotlib.image as mpimg\n",
        "import h5py"
      ],
      "metadata": {
        "id": "gvV0iEDBRCIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize some of the data we have"
      ],
      "metadata": {
        "id": "jQqakusncaLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# megadjuk, hogy hány sor és oszlop jelenjen meg\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 3, nrows * 3)\n",
        "\n",
        "next_benign_pix = [key for key in benign_ids[:int(ncols*nrows/2)]]\n",
        "next_malignant_pix = [key for key in malignant_ids[:int(ncols*nrows/2)]]\n",
        "\n",
        "with h5py.File('train-image.hdf5', 'r') as f:\n",
        "  for i, img_key in enumerate(next_benign_pix + next_malignant_pix):\n",
        "    image_data = f[img_key][()]\n",
        "    image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "    sp = plt.subplot(nrows, ncols, i + 1)\n",
        "    plt.imshow(image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QZ5a4wS5OzNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to train another model on the metadata.  \n",
        "The if both models say true, then the leisure is probably malignant.  \n",
        "Here we select the relevant metadata for that model."
      ],
      "metadata": {
        "id": "BegGV7m3-5OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COLUMNS = [\n",
        "    'clin_size_long_diam_mm',\n",
        "    'tbp_lv_areaMM2',\n",
        "    'tbp_lv_area_perim_ratio',\n",
        "    'tbp_lv_color_std_mean',\n",
        "    'tbp_lv_deltaLBnorm',\n",
        "    'tbp_lv_minorAxisMM',\n",
        "    'tbp_lv_perimeterMM'\n",
        "]\n",
        "\n",
        "malignant_data[COLUMNS].head()"
      ],
      "metadata": {
        "id": "1VSWvJiW1TV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saple the ids to get a sample data.  \n",
        "We will basically use it to \"test\" each model before training it on huge data."
      ],
      "metadata": {
        "id": "3tGBNH48_KtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sample_size = 10000\n",
        "\n",
        "benign_sample_ids = random.sample(train_benign_ids, k=sample_size)\n",
        "malignant_metadata = random.sample(train_malignant_ids, k=int(sample_size/1000))\n",
        "\n",
        "print(f'Benign sample ids: {benign_sample_ids[:5]}')\n",
        "print(f'Malignant sample ids: {malignant_metadata[:5]}')\n",
        "\n",
        "print(f'Benign sample size: {len(benign_sample_ids)}')\n",
        "print(f'Malignant sample size: {len(malignant_metadata)}')"
      ],
      "metadata": {
        "id": "syo_AP9c7nwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# encoder rész\n",
        "encoder = keras.models.Sequential([\n",
        "    keras.layers.Reshape([96, 96, 3], input_shape=[96, 96, 3]),\n",
        "    keras.layers.Conv2D(36, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "    keras.layers.Conv2D(72, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "    keras.layers.Conv2D(144, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(pool_size=2)\n",
        "])\n",
        "\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "q8MBUDgfrQH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder rész\n",
        "decoder = keras.models.Sequential([\n",
        "    keras.layers.Conv2DTranspose(72, kernel_size=(3, 3), strides=2, padding=\"same\", activation=\"relu\",\n",
        "                                 input_shape=[12, 12, 144]),\n",
        "    keras.layers.Conv2DTranspose(36, kernel_size=(3, 3), strides=2, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2DTranspose(3, kernel_size=(3, 3), strides=2, padding=\"same\"),\n",
        "    keras.layers.Reshape([96, 96, 3])\n",
        "])\n",
        "\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "QCuitd2vtNUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modell a teljes AE-re\n",
        "autoencoder = keras.models.Sequential([encoder, decoder])\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')"
      ],
      "metadata": {
        "id": "zt5irUnSAkOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def hdf5_generator(hdf5_path, image_ids, batch_size, target_size=(96, 96)):\n",
        "    \"\"\"\n",
        "    Generates batches of image data from an HDF5 file.\n",
        "\n",
        "    Args:\n",
        "        hdf5_path (str): Path to the HDF5 file.\n",
        "        image_ids (list): List of image IDs to load.\n",
        "        batch_size (int): Number of images per batch.\n",
        "        target_size (tuple): Target size for resizing images.\n",
        "\n",
        "    Yields:\n",
        "        tuple: A batch of images and labels.\n",
        "    \"\"\"\n",
        "    with h5py.File(hdf5_path, 'r') as f:\n",
        "        num_images = len(image_ids)\n",
        "        while True:\n",
        "            for i in range(0, num_images, batch_size):\n",
        "                batch_ids = image_ids[i: i + batch_size]\n",
        "                images = []\n",
        "                for image_id in batch_ids:\n",
        "                    image_data = f[image_id][()]\n",
        "                    image = Image.open(io.BytesIO(image_data))\n",
        "                    image = image.resize(target_size)\n",
        "                    image = np.array(image)\n",
        "                    image = image / 255.0\n",
        "                    images.append(image)\n",
        "\n",
        "                images = np.array(images)\n",
        "\n",
        "                yield images, images"
      ],
      "metadata": {
        "id": "DMXIi1CIAysy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = 2000\n",
        "batch_size = 128\n",
        "train_len = steps_per_epoch * batch_size\n",
        "\n",
        "print(f'Steps per epoch: {steps_per_epoch}')\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'Train len: {train_len}')\n",
        "\n",
        "train_generator = hdf5_generator('train-image.hdf5', train_benign_ids[:train_len], batch_size=batch_size)\n",
        "valid_generator = hdf5_generator('train-image.hdf5', valid_benign_ids, batch_size=batch_size)\n",
        "\n",
        "autoencoder.fit(train_generator,\n",
        "                epochs = 1,\n",
        "                steps_per_epoch = steps_per_epoch,\n",
        "                batch_size = batch_size,\n",
        "                shuffle = True)"
      ],
      "metadata": {
        "id": "wAlRQyAtAzmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a function that does the following: plot an image an a prediction of the autoencoder where the image is from bengin ids. to a third column also plot the length of the differences in each pixel to a heatmap (be aware that after subtracting a vector from an other, it does not give the length, just the difference. what i want is the length)\n",
        "\n",
        "def plot_image_and_prediction(benign_id):\n",
        "  \"\"\"\n",
        "  Plots an image, its autoencoder prediction, and a heatmap of pixel differences.\n",
        "\n",
        "  Args:\n",
        "      benign_id (str): The ID of the benign image to process.\n",
        "      autoencoder (keras.Model): The trained autoencoder model.\n",
        "  \"\"\"\n",
        "\n",
        "  with h5py.File('train-image.hdf5', 'r') as f:\n",
        "    image_data = f[benign_id][()]\n",
        "    image = Image.open(io.BytesIO(image_data))\n",
        "    image = image.resize((96, 96))\n",
        "    image = np.array(image)\n",
        "    image = image / 255.0\n",
        "\n",
        "    # Get the prediction\n",
        "    prediction = autoencoder.predict(np.expand_dims(image, axis=0), verbose=0)[0]\n",
        "\n",
        "    # Calculate the pixel difference and length\n",
        "    diff = image - prediction\n",
        "    diff_length = np.linalg.norm(diff, axis=2)  # Calculate length of difference in each pixel\n",
        "\n",
        "\n",
        "    # Plot the image and prediction side by side\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axs[0].imshow(image)\n",
        "    axs[0].set_title('Original Image')\n",
        "    axs[1].imshow(prediction)\n",
        "    axs[1].set_title('Autoencoder Prediction')\n",
        "    im = axs[2].imshow(diff_length, cmap='hot')\n",
        "    axs[2].set_title('Difference Length (Heatmap)')\n",
        "    fig.colorbar(im, ax=axs[2])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a trained autoencoder model named 'autoencoder'\n",
        "# and a list of benign IDs, e.g., 'benign_ids'\n",
        "# plot_image_and_prediction(benign_ids[0], autoencoder)\n"
      ],
      "metadata": {
        "id": "PPcNBQk5mtPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ids = random.sample(train_benign_ids, k=3)\n",
        "\n",
        "for image_id in sample_ids:\n",
        "  plot_image_and_prediction(image_id)"
      ],
      "metadata": {
        "id": "XCfqU9H1ndM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "malignant_sample_ids = random.sample(train_malignant_ids, k=3)\n",
        "\n",
        "for image_id in malignant_sample_ids:\n",
        "  plot_image_and_prediction(image_id)"
      ],
      "metadata": {
        "id": "kQgPeHGPnpzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a function that creates a 2 dimensional heatmap from an original and a predicted image\n",
        "\n",
        "def create_heatmap(original_image, predicted_image):\n",
        "  diff = original_image - predicted_image\n",
        "  diff_length = np.linalg.norm(diff, axis=2)  # Calculate length of difference in each pixel\n",
        "  return diff_length\n"
      ],
      "metadata": {
        "id": "x0r8gZcxJyOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_center_bitmask(heatmap_shape, center, radius):\n",
        "  \"\"\"Creates a bitmask for the center of a heatmap.\n",
        "\n",
        "  Args:\n",
        "    heatmap_shape (tuple): The shape of the heatmap (height, width).\n",
        "    center (tuple): The coordinates of the center (cx, cy).\n",
        "    radius (int): The radius of the circular center region.\n",
        "\n",
        "  Returns:\n",
        "    numpy.ndarray: A Boolean array representing the bitmask.\n",
        "  \"\"\"\n",
        "  height, width = heatmap_shape\n",
        "  cx, cy = center\n",
        "  x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
        "  distances = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
        "  mask = distances <= radius\n",
        "  return mask"
      ],
      "metadata": {
        "id": "o9oqF6spORL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_error(image_id):\n",
        "  with h5py.File('train-image.hdf5', 'r') as f:\n",
        "    image_data = f[image_id][()]\n",
        "    image = Image.open(io.BytesIO(image_data))\n",
        "    image = image.resize((96, 96))\n",
        "    image = np.array(image)\n",
        "\n",
        "    prediction = autoencoder.predict(np.expand_dims(image, axis=0), verbose=0)[0]\n",
        "\n",
        "    heatmap = create_heatmap(image, prediction)\n",
        "    blue_channel = image[:, :, 2]\n",
        "    blue_channel[blue_channel == 0] = 1000.0\n",
        "    heatmap = heatmap / blue_channel\n",
        "\n",
        "    bitmask = create_center_bitmask(heatmap.shape, (48, 48), 20)\n",
        "    heatmap = heatmap * bitmask\n",
        "    error = np.mean(heatmap)\n",
        "\n",
        "    return error"
      ],
      "metadata": {
        "id": "_LRWpHoE5OAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: fill a numpy array with the errors for bengin and an other one for malignant test images\n",
        "\n",
        "benign_errors = []\n",
        "malignant_errors = []\n",
        "\n",
        "for image_id in test_benign_ids[:1000]:\n",
        "  try:\n",
        "    error = calculate_error(image_id)\n",
        "    benign_errors.append(error)\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing benign image {image_id}: {e}\")\n",
        "\n",
        "for image_id in test_malignant_ids:\n",
        "  try:\n",
        "    error = calculate_error(image_id)\n",
        "    malignant_errors.append(error)\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing malignant image {image_id}: {e}\")\n",
        "\n",
        "benign_errors_np = np.array(benign_errors)\n",
        "malignant_errors_np = np.array(malignant_errors)"
      ],
      "metadata": {
        "id": "QIyUFQpfEaRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Mean benign error: {np.mean(benign_errors_np)}')\n",
        "print(f'Mean malignant error: {np.mean(malignant_errors_np)}')"
      ],
      "metadata": {
        "id": "NwLpfWZZQumw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a box plot for benign_errors_np and malignant_errors_np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.boxplot([benign_errors_np, malignant_errors_np], labels=['Benign', 'Malignant'])\n",
        "plt.title('Box Plot of Reconstruction Errors')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NYnVdPt8E9uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a box plot for benign_errors_np and malignant_errors_np and remove the very big outliers\n",
        "\n",
        "# Remove outliers beyond 3 standard deviations\n",
        "benign_errors_no_outliers = benign_errors_np[(np.abs(benign_errors_np - np.mean(benign_errors_np)) < 3 * np.std(benign_errors_np))]\n",
        "malignant_errors_no_outliers = malignant_errors_np[(np.abs(malignant_errors_np - np.mean(malignant_errors_np)) < 3 * np.std(malignant_errors_np))]\n",
        "\n",
        "# Create the box plot without outliers\n",
        "plt.boxplot([benign_errors_no_outliers, malignant_errors_no_outliers], labels=['Benign', 'Malignant'])\n",
        "plt.title('Box Plot of Reconstruction Errors (Outliers Removed)')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J6xWO3KbPYkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot the confusion matrix if we predict that an image is malignant if the error is above a threshold (use benign_errors and malignant_errors)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "threshold = 0.1  # Adjust this threshold as needed\n",
        "\n",
        "y_true = [0] * len(benign_errors) + [1] * len(malignant_errors)\n",
        "y_pred = [(1 if error > threshold else 0) for error in benign_errors] + [(1 if error > threshold else 0) for error in malignant_errors]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DzaeW64mHle6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the f1 score for our predictions\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming y_true and y_pred are already defined as in your code\n",
        "\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "u39cqTthIdoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPT43I8iE1I9BVThbZeSggL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}